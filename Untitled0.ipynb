{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raeFPKkMMEJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Boosting? How does it improve weak learners?\n",
        "\n",
        "Ans Boosting is an ensemble technique that builds a strong model by combining many weak learners (typically shallow decision trees). Models are trained sequentially; each new learner focuses more on samples the previous ones misclassified (by reweighting data or fitting the negative gradient of a loss). The final prediction is a weighted sum/majority vote of all learners, which reduces bias, controls variance, and improves generalization."
      ],
      "metadata": {
        "id": "ym8e6y0sMGqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.   What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "\n",
        "Ans.AdaBoost: reweights training samples each round based on classification errors; the next stump/tree pays more attention to misclassified points. Optimizes an exponential loss implicitly via sample weights.\n",
        "\n",
        "Gradient Boosting: fits each new tree to the negative gradient (residuals) of a specified loss (e.g., squared error, deviance). It’s a general gradient-descent framework in function space with learning rate and tree parameters controlling steps."
      ],
      "metadata": {
        "id": "lapCIYFyMKtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does regularization help in XGBoost?\n",
        "\n",
        "Ans. XGBoost regularizes at multiple levels:\n",
        "\n",
        "L1/L2 penalties on leaf weights (controls model complexity).\n",
        "\n",
        "Shrinkage (learning_rate) to take smaller, safer steps.\n",
        "\n",
        "Subsampling of rows/columns to decorrelate trees and cut variance.\n",
        "\n",
        "Tree constraints (max_depth, min_child_weight, gamma) to prune weak splits.\n",
        "Together these prevent overfitting and improve out-of-sample performance and training stability."
      ],
      "metadata": {
        "id": "wHLE8nyUMQxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Ans. CatBoost uses ordered target statistics and ordered boosting to transform categories while avoiding target leakage. It handles high-cardinality features with efficient encodings, supports missing values natively, and uses symmetric trees for fast, robust training—often requiring minimal preprocessing compared to other boosters."
      ],
      "metadata": {
        "id": "jl8SlMEgMV6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "\n",
        "Ans. Credit default & fraud detection (imbalanced, heterogeneous features)\n",
        "\n",
        "Click-through rate / ad ranking\n",
        "\n",
        "Medical diagnosis (tabular clinical features)\n",
        "\n",
        "Customer churn / propensity modeling\n",
        "\n",
        "Forecasting tabular regression (e.g., house prices, demand)\n",
        "Boosters shine on structured/tabular data with complex, non-linear interactions and mixed feature types"
      ],
      "metadata": {
        "id": "TY4M2QdMMZXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "7ky6nOQbMc3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "EIUTLHKbMdtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "1DqdBKtZMfp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "print(\"R^2:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "9jpchWqqMh-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Wkc0QGESMlsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.2]},\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_tr, y_tr)\n",
        "best = grid.best_estimator_\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_te, best.predict(X_te)))\n"
      ],
      "metadata": {
        "id": "bxM0MpNBMnYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "wH4Ev8fqMo8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    depth=6, learning_rate=0.1, iterations=300,\n",
        "    loss_function='Logloss', verbose=0, random_seed=42\n",
        ")\n",
        "model.fit(X_tr, y_tr)\n",
        "pred = model.predict(X_te)\n",
        "\n",
        "cm = confusion_matrix(y_te, pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['malignant','benign'], yticklabels=['malignant','benign'])\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "usoj4sR5MrOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bro43w9AMtGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8CJtn_C8L_uF"
      }
    }
  ]
}